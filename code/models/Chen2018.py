import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import accuracy_score
from keras import backend as K

from .Basics import MachineLearningModel

class InceptionV1(MachineLearningModel):

    def __init__(
        self,
        path,
        ext='png',
        height=-1,
        width=-1,
        channels=1,
        max_epochs=5000,
        patience=50,
        batch_size=1,
        learning_rate=1e-3,
        lr_decay=1e-1,
        lr_treshold=1e-7
    ):
        """
        Parameters
        ----------
        path : str
            Path to malware dataset
        ext : str
            File extension of dataset files (default 'png')
        height : int
            Height of the malware image (default -1 [We wanna use original dimensions])
        width : int
            Width of the malware image (default -1 [We wanna use original dimensions])
        channels : int
            Number of image's channels (default 1)
        max_epochs : int
            Number of max epochs to be run (default 5000)
        patience : int
            Early stopping criteria. If improvement is not made for <patience> epochs (default 10)
        batch_size : int
            Batch size (default 1 [Images have different dimensions])
        learning_rate : float
            Model's learning rate (default 1e-3)
        lr_decay : float
            Model's learning rate decay parameter (default 1e-1)
        lr_treshold : float
            Model's learning rate treshold to stop training (default 1e-7)
        """

        # We save the epoch where the best acc occurs (early stopping)
        self.best_epoch = -1

        self.height = height
        self.width = width
        self.channels = channels
        self.max_epochs = max_epochs
        self.initial_patience = patience
        self.patience = patience
        self.batch_size = batch_size
        self.lr_initial = learning_rate
        self.lr_training = learning_rate
        self.lr_decay = lr_decay
        self.lr_treshold = lr_treshold

        # Init parent class
        MachineLearningModel.__init__(self, path, 'model_inceptionv3_', ext)

    def reset_learning_rate(self):
        """
        Set Learning Rate to initial value.
        """

        self.lr_training = self.lr_initial

        return

    def build_graph(self, input_shape, output_shape):
        """
            Build tensorflow graph.
        """
        print("*** Building tensorflow Graph.")
        print("================================================")
        K.set_learning_phase(0)

        # Some GPUs have memory issues running a model this big. It might help
        # limiting the memory usage per process
        # IMPORTANT: models have to be loaded AFTER SETTING THE SESSION for keras!
        # Otherwise, their weights will be unavailable in the threads after the session there has been set
        # https://github.com/tensorflow/tensorflow/issues/28287
        config = tf.ConfigProto()
        config.gpu_options.per_process_gpu_memory_fraction = 0.7
        K.set_session(tf.Session(config=config))


        ##################
        ### CNN layers ###
        ##################
        self.base_model = tf.keras.applications.InceptionV3(
            input_shape=(self.height, self.width, 3),
            include_top=False,
            weights='imagenet',
            pooling='max'
        )

        # Freezing weights update
        self.base_model.trainable = False
        K.set_learning_phase(1)


        # base_model.summary()

        ####################
        ### Output layer ###
        ####################

        self.model = tf.keras.Sequential([
            self.base_model,
            #keras.layers.Dense(1024, activation='relu'),
            keras.layers.Dense(self.n_classes, activation='softmax')
        ])

        self.model.compile(
            optimizer=tf.keras.optimizers.Adam(lr=self.lr_initial),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        self.model.summary()

        # Input
        # self.model_output = self.model(self.model_X)

        # Compute Loss
        # self.model_loss = tf.losses.softmax_cross_entropy(self.model_y_one_hot, self.model_output)
        # self.model_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.model_y_one_hot, logits=self.model_output)
        # self.model_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(self.model_y_one_hot, self.model_output))

        # Optimizer with learning rate decay
        # self.model_optimizer = tf.train.AdamOptimizer(self.model_learning_rate).minimize(self.model_loss)
        # self.model_optimizer = tf.train.GradientDescentOptimizer(self.model_learning_rate).minimize(self.model_loss)
        # self.model_optimizer = tf.train.RMSPropOptimizer(self.model_learning_rate).minimize(self.model_loss)

        # Accuracy
        # prediction = tf.argmax(self.model_output, 1, name="net_output")
        # correct_prediction = tf.equal(prediction, self.model_y)
        # self.model_correct = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))

        print("================================================")

        return

    def train(self, n_folds=1, split_method='nataraj', use_subset=False):
        self.logger.info("** Training started!")

        self.data_handler.split_dataset(n_folds=n_folds, split_method=split_method, use_subset=use_subset)
        self.n_classes = self.data_handler.n_classes

        self.logger.info("** Fitting {} classes!".format(self.n_classes))

        avg_acc = []
        test_acc = 0
        for fold in range(n_folds):
            self.logger.info("===== FOLD {} =====".format(fold))

            self.data_handler.compute_fold_split()

            self.patience = self.initial_patience
            self.build_graph(
                (None, self.height, self.width, 3),
                self.n_classes
            )
            self._train(fold)

            # Restart training with best epoch
            self.model.load_weights(self.get_tmp_model_path() + str(self.best_epoch) + '_f' + str(fold))
            for layer in self.model.layers:
                layer.trainable = True
            self.model.compile(
                optimizer=tf.keras.optimizers.Adam(lr=self.lr_initial),
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
            self.model.summary()
            self.patience *= 3
            self._train(fold, self.best_epoch)

            # Test with unseen data
            test_data = self.data_handler.get_test_data()
            test_acc = self._test(
                            fold+1,
                            self.get_tmp_model_path() + str(self.best_epoch) + '_f' + str(fold),
                            test_data
                        )
            avg_acc.append(test_acc)
        print_str = "** Finished training! Average ACC: {}; STD DEV {};".format(
                        np.mean(avg_acc),
                        np.std(avg_acc)
                    )
        self.logger.warning(print_str)
        self.data_handler.save_to_log(print_str  + '\n')

    def _train(self, fold, epoch_range_start=0):

        train_data = self.data_handler.get_train_data()
        training_set_size = len(train_data)
        n_batches = np.ceil(training_set_size/self.batch_size)
        best_acc = 0
        best_loss = 999999
        distance_from_improvement = 0

        # Run epochs
        for epoch in range(epoch_range_start, self.max_epochs):
            self.logger.info("Epoch {}".format(epoch+1))

            # Mini-batch gradient descent
            np.random.shuffle(train_data)
            train_acc = 0
            train_loss = 0

            for idx in range(0, training_set_size, self.batch_size):

                batch_X, batch_y = self.data_handler.load_raw_images(
                    train_data[idx:idx+self.batch_size],
                    self.height,
                    self.width,
                    self.channels,
                    True # Stack image channels (RGB)
                )

                #self.logger.info("Data info: {} : {}".format(batch_X.shape, batch_y.shape))

                ret = self.model.train_on_batch(
                    batch_X,
                    tf.keras.utils.to_categorical(batch_y, self.n_classes)
                )

                # print(ret)
                # print(ret.shape)
                train_loss += ret[0]
                train_acc += ret[1]
            validation_acc, validation_loss = self._validate()

            self.logger.info(
                "*TRAINING* SET => ACC: {:.4f} ; " \
                "LOSS: {:.4f}"
                .format(
                    train_acc/n_batches,
                    train_loss/n_batches
                )
            )
            self.logger.info(
                "*VALIDATE* SET => ACC: {:.4f} ; " \
                "LOSS: {:.4f}"
                .format(
                    validation_acc,
                    validation_loss
                )
            )
            self.data_handler.save_to_log(
                "{}, {}, {}, {}, {}\n"
                .format(
                    (epoch+1),
                    train_acc/training_set_size,
                    train_loss/training_set_size,
                    validation_acc,
                    validation_loss
                )
            )

            ###############################
            ###      EARLY STOPPING     ###
            ###############################
            if validation_acc > best_acc:
                best_loss = validation_loss
                best_acc = validation_acc
                self.best_epoch = (epoch+1)
                distance_from_improvement = 0
                # Save network parameters
                self.model.save_weights(
                    self.get_tmp_model_path() + str(epoch+1) + '_f' + str(fold)
                )
            else:
                distance_from_improvement += 1

            if distance_from_improvement > self.patience:

                print_str = "***** No improvement in last {} epochs! "\
                "Stopping. Best epoch: {} => ACC: {}; LOSS: {};" \
                .format(
                    self.patience,
                    self.best_epoch,
                    best_acc,
                    best_loss,
                )
                self.logger.warning(print_str)
                self.data_handler.save_to_log(print_str + '\n')
                break

        return

    def _validate(self):

        validation_acc = 0
        validation_loss = 0
        validation_data = self.data_handler.get_validation_data()
        validation_set_size = len(validation_data)
        n_batches = np.ceil(validation_set_size/self.batch_size)

        for idx in range(0, validation_set_size, self.batch_size):

            validation_X, validation_y = self.data_handler.load_raw_images(
                validation_data[idx:idx+self.batch_size],
                self.height,
                self.width,
                self.channels,
                True # Stack image channels (RGB)
            )

            ret = self.model.test_on_batch(
                validation_X,
                tf.keras.utils.to_categorical(validation_y, self.n_classes)
            )

            validation_loss += ret[0]
            validation_acc += ret[1]

        return validation_acc/n_batches, validation_loss/n_batches

    def _test(self, fold, model_path, test_data):

        acc = 0
        self.logger.warning("*** Loading tensorflow Graph from {}.".format(model_path))

        result_file = self.data_handler.create_file(
            name="test_results_{}.txt".format(fold),
        )

        test_set_size = len(test_data)
        labels = []
        predictions = []

        self.model.load_weights(model_path)

        for idx in range(0, test_set_size, self.batch_size):
            test_X, test_y = self.data_handler.load_raw_images(
                test_data[idx:idx+self.batch_size],
                self.height,
                self.width,
                self.channels,
                True # Stack image channels (RGB)
            )

            y_predict = self.model.predict_on_batch(test_X)

            for i in range(len(test_y)):
                labels.append(test_y[i])
                predictions.append(np.argmax(y_predict[i]))
                result_file.write("{}, {}\n".format(test_y[i], np.argmax(y_predict[i])))

        acc = accuracy_score(labels, predictions)
        print_str = "\t\t *TEST* SET => ACC: {:.4f}\n".format(acc)
        self.logger.info(print_str)
        result_file.write(print_str)

        self.logger.info("** Testing finished!")
        return acc

    def test(self, path):
        self.logger.info("[*] Running in test only mode.")

        self.data_handler.split_dataset()
        self.n_classes = self.data_handler.n_classes
        self.data_handler.compute_fold_split()

        self.logger.info("** Fitting {} classes!".format(self.n_classes))


        self.build_graph(
            (None, self.height, self.width, 3),
            self.n_classes
        )

        self._test('test_only', path, self.data_handler.get_all_data())
